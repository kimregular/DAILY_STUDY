언어는 정보를 서로 소통하기 위해 만들어졌다. 컴퓨터에게 명령을 내리는 일이 프로그래머가 하는 일이다. 그런데 컴퓨터는 사람의 말을 알아듣지 못한다. 따라서 사람이 컴퓨터의 말을 배워야한다. 다행히도 자연어와 컴퓨터 언어는 쓰인 기호나 사용법 등의 요소를 상당 부분 공유한다. 단지 다른점은 컴퓨터 언어는 항상 문자 언어만 사용한다는 점이다.

컴퓨터는 0과 1로 모든 정보를 표현하고, 0과 1로 표현된 정보만을 이해할 수 있다. 0과 1밖에 모르는 컴퓨터가 1보다 큰 수를 어떻게 계산할 수 있을까? 그러니까 `3 + 4` 같은 계산은 어떻게 하는걸까?

우선 컴퓨터가 이해하는 가장 작은 정보 단위를 알아보자. 다시말하지만 컴퓨터는 0 또는 1밖에 이해하지 못한다. 0과 1을 나타내는 가장 작은 정보 단위를 `비트(bit)` 라고 부른다. 비트[^1]는 전구에 빗대어 생각할 수 있다. 전구 한 개는 꺼짐과 켜짐을 표현할 수 있다. 따라서 1비트는 0 또는 1, 두 가지 정보를 표현할 수 있다.

그럼 2비트는 몇 개의 정보를 표현할 수 있을까? 직접 계산해보자.

![‎데이터.‎001](file:///Users/regularkim/Library/Mobile%20Documents/com~apple~CloudDocs/Typora/%E1%84%82%E1%85%A9%E1%84%89%E1%85%A7%E1%86%AB%E1%84%8B%E1%85%A5%E1%86%B8%E1%84%85%E1%85%A9%E1%84%83%E1%85%B3%E1%84%8B%E1%85%AD%E1%86%BC/CS/%E2%80%8E%E1%84%83%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%90%E1%85%A5.%E2%80%8E001.jpeg?lastModify=1694357964)

2비트는 총 4가지의 정보를 표현할 수 있다.

그럼 3비트는 어떨까?
![[‎데이터.‎002.jpeg]]
여덟 가지 상태를 표현할 수 있다.

정리하자면 n개의 전구로 표현할 수 있는 상태는 $2^n$ 가지이다. 전구가 비트이므로 당연히 n비트는 $2^n$ 가지 정보를 표현할 수 있다.

웹 브라우저, 워드 프로세서, 포토샵 등 모든 프로그램은 수십만, 수백만 개 이상의 0과 1로 이루어져 있다. 다시 말해서 우리가 실행하는 프로그램은 수십만 비트, 수백만 비트로 이루어져있다. 단위가 너무 크기 때문에 편의를 위해 비트보다 큰 단위를 사용한다. 이러한 단위들이 바로 `바이트`, `킬로바이트`, `메가바이트`, `기가바이트`, `` `테라바이트 `` 등이다.

`바이트(byte)` 는 여덟 개의 비트를 묶은 단위이다. 비트보다 한 단계 큰 단위이다. 하나의 바이트는 28, 즉 256개의 정보를 표현할 수 있다.

바이트 또한 더 큰 단위로 묶을 수 있다. 1바이트 1000개를 묶은 단위를 1 `킬로바이트(kilobyte : kb)` 라고 부른다. 그리고 1킬로바이트 1000개를 묶은 단위를 1 `메가바이트(megabyte : mb)` 라고 부른다. 1메가 바이트를 1000개 묶은 단위를 1 `기가바이트(gigabyte : gb)`, 1기가바이트 1000개를 묶은 단위를 1 `테라바이트(terabyte : tb)` 라고 부른다. 바이트를 제외한 kb, mb, gb, 그 이상의 단위들은 모두 이전 단위를 1000개 묶어 표현한 단위이다.

---

[^1]: 비트라는 단어는 2진법을 사용한다는 뜻의 `바이너리(binary)` 와 숫자를 뜻하는 `디지트(digit)` 가 합쳐진 말이다.